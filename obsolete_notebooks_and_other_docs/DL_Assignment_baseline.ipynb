{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xveQIshNWYdc"
   },
   "source": [
    "# 0. Loading Dataset\n",
    "\n",
    "You can download the dataset from {https://darwin.v7labs.com/v7-labs/covid-19-chest-x-ray-dataset?sort=priority\\%3Adesc}.\n",
    "The data entitled as '`darwin dataset pull v7-labs/covid-19-chest-x-ray-dataset:all-images`' will be used in this assignment. All dataset consist of 6504 images from 702 classes. We will extract the images of 4 classes (Bacterial Pneumonia, Viral Pneumonia, No Pneumonia (healthy), Covid-19) and save them as .npy file with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I6bqTVRSs-sw",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 06:23:27.401136: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 06:23:28.609245: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-04 06:23:28.609312: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-04 06:23:32.272777: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 06:23:32.273523: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 06:23:32.273543: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "All packages and modules are imported in the \"functions.py\" file located next to this notebook\n",
    "\n",
    "Additionally a number of code chunks employed in previous versions of this notebook have been\n",
    "defined into functions and placed in that same file in order to simplify the exposition of the \n",
    "main points and results of the assignment. For further insight on the code behind the actions\n",
    "here executed this \"functions.py\" file can be visited, there all the details can be found.\n",
    "\n",
    "IMPORTANT: for the appropriate functioning of this notebook it must be placed in the same \n",
    "directory as \"functions.py\"\n",
    "'''\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# > DISABLED - ALREADY EXECUTED < #\n",
    "###################################\n",
    "\n",
    "\n",
    "# # all-images file should be uploaded to the same file\n",
    "# imageNames = glob.glob(\"all-images/*\")\n",
    "# \n",
    "# dataset = []\n",
    "# labels = []\n",
    "# \n",
    "# for i, imName in enumerate(imageNames):\n",
    "# \n",
    "#     # Opening JSON file\n",
    "#     f = open(imName)\n",
    "#     data = json.load(f)\n",
    "#     for j in range(len(data['annotations'])):\n",
    "# \n",
    "#         if 'COVID-19' in (data['annotations'][j]['name']):\n",
    "#           #load images from url    \n",
    "#             urllib.request.urlretrieve(data['image']['url'],\"img.png\")    \n",
    "#             img = Image.open(\"img.png\")\n",
    "#             #convert images to grayscale\n",
    "#             imgGray = img.convert('L')\n",
    "#             #resize the image (156x156)\n",
    "#             im = imgGray.resize((156,156), Image.LANCZOS)           \n",
    "#             label = data['annotations'][j]['name']\n",
    "#             dataset.append(np.array(im))\n",
    "#             labels.append(label)\n",
    "#             print(label)\n",
    "#             break\n",
    "# \n",
    "#         if 'Viral Pneumonia' in (data['annotations'][j]['name']) \\\n",
    "#             or 'Bacterial Pneumonia' in (data['annotations'][j]['name']) \\\n",
    "#             or 'No Pneumonia (healthy)' in (data['annotations'][j]['name']):\n",
    "#             #load images from url    \n",
    "#             urllib.request.urlretrieve(data['image']['url'],\"img.png\")    \n",
    "#             img = Image.open(\"img.png\")\n",
    "#             #convert images to grayscale\n",
    "#             imgGray = img.convert('L')\n",
    "#             #resize the image (156x156)\n",
    "#             im = imgGray.resize((156,156), Image.LANCZOS)           \n",
    "#             label = data['annotations'][j]['name']\n",
    "#             dataset.append(np.array(im))\n",
    "#             labels.append(label)\n",
    "#             break\n",
    "# \n",
    "# #Convert data shape of (n_of_samples, width, height, 1)\n",
    "# dataset = np.dstack(dataset)    \n",
    "# dataset = np.rollaxis(dataset,-1)\n",
    "# labels = np.array(labels)\n",
    "# \n",
    "# #convert images gray scale to rgb\n",
    "# data = np.array(layers.Lambda(tf.image.grayscale_to_rgb)(tf.expand_dims(dataset, -1)))\n",
    "# \n",
    "# # save data and labels into a folder\n",
    "# np.save(\"data.npy\", data)\n",
    "# np.save(\"labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn3sH7yJWboe"
   },
   "source": [
    "Once you save your data, you can load it from your directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b6ZPDhVLWbyq"
   },
   "outputs": [],
   "source": [
    "features = np.load('data/data.npy')\n",
    "labels = np.load('data/labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1b7dP-VbJpC"
   },
   "source": [
    "# 1. Data preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Further split the training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-r4ta3MbKLJ"
   },
   "source": [
    "### 1.2 Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LeplL77mbKS4"
   },
   "outputs": [],
   "source": [
    "# Float conversion to allow normalization\n",
    "X_train=X_train.astype('float32')\n",
    "X_test=X_test.astype('float32')\n",
    "X_val=X_val.astype('float32')\n",
    "\n",
    "# Normalization  \n",
    "X_train=X_train/255.0\n",
    "X_test=X_test/255.0\n",
    "X_val=X_val/255.0\n",
    "\n",
    "# Compute the mean and standard deviation of the training set for standardization (NOT USED)\n",
    "train_mean = np.mean(X_train, axis=0)\n",
    "train_std = np.std(X_train, axis=0)\n",
    "\n",
    "# Standardization (NOT USED)\n",
    "X_train_norm = (X_train - train_mean) / train_std\n",
    "X_val_norm = (X_val - train_mean) / train_std\n",
    "X_test_norm = (X_test - train_mean) / train_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary that maps each category to a numerical value\n",
    "label_map = {\"Bacterial Pneumonia\": 0, \"Viral Pneumonia\": 1, \"No Pneumonia (healthy)\": 2, \"COVID-19\": 3}\n",
    "\n",
    "# Encode the categorical labels as numerical values using the label map\n",
    "y_train_encoded = np.vectorize(label_map.get)(y_train)\n",
    "y_val_encoded = np.vectorize(label_map.get)(y_val)\n",
    "y_test_encoded = np.vectorize(label_map.get)(y_test)\n",
    "\n",
    "# Convert the numerical labels to one-hot encoded format\n",
    "num_classes = 4\n",
    "y_train_onehot = keras.utils.to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_val_onehot = keras.utils.to_categorical(y_val_encoded, num_classes=num_classes)\n",
    "y_test_onehot = keras.utils.to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YONVgtOAbKca"
   },
   "source": [
    "# 2. Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "LhUlV9UNbKiH",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "128/128 [==============================] - 645s 5s/step - loss: 0.7968 - accuracy: 0.6746 - val_loss: 0.6137 - val_accuracy: 0.7390\n",
      "Epoch 2/10\n",
      "128/128 [==============================] - 536s 4s/step - loss: 0.6372 - accuracy: 0.7391 - val_loss: 0.6423 - val_accuracy: 0.7312\n",
      "Epoch 3/10\n",
      "128/128 [==============================] - 473s 4s/step - loss: 0.5594 - accuracy: 0.7702 - val_loss: 0.5811 - val_accuracy: 0.7586\n",
      "Epoch 4/10\n",
      "128/128 [==============================] - 373s 3s/step - loss: 0.5050 - accuracy: 0.7939 - val_loss: 0.5608 - val_accuracy: 0.7615\n",
      "Epoch 5/10\n",
      "128/128 [==============================] - 374s 3s/step - loss: 0.4573 - accuracy: 0.8144 - val_loss: 0.6046 - val_accuracy: 0.7576\n",
      "Epoch 6/10\n",
      "128/128 [==============================] - 369s 3s/step - loss: 0.4232 - accuracy: 0.8262 - val_loss: 0.5628 - val_accuracy: 0.7889\n",
      "Epoch 7/10\n",
      "128/128 [==============================] - 359s 3s/step - loss: 0.3493 - accuracy: 0.8567 - val_loss: 0.5934 - val_accuracy: 0.7634\n",
      "Epoch 8/10\n",
      "128/128 [==============================] - 399s 3s/step - loss: 0.2887 - accuracy: 0.8804 - val_loss: 0.7404 - val_accuracy: 0.7693\n",
      "Epoch 9/10\n",
      "128/128 [==============================] - 385s 3s/step - loss: 0.2571 - accuracy: 0.8978 - val_loss: 0.7557 - val_accuracy: 0.7468\n",
      "Epoch 10/10\n",
      "128/128 [==============================] - 385s 3s/step - loss: 0.1821 - accuracy: 0.9306 - val_loss: 0.9375 - val_accuracy: 0.7498\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "def build_baseline_model():\n",
    "    model = keras.Sequential([\n",
    "        # Convolutional layers\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(156, 156, 3)),\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Dense layers\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model with appropriate loss function, optimizer, and metrics\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = build_baseline_model()\n",
    "\n",
    "# Train the model for 10 epochs with a batch size of 32\n",
    "history = baseline_model.fit(\n",
    "    X_train_norm,\n",
    "    y_train_onehot,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_norm, y_val_onehot)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_all_val_acc = np.mean(history.history[\"val_accuracy\"])\n",
    "baseline_all_val_loss = np.mean(history.history[\"val_loss\"])\n",
    "baseline_all_train_acc = np.mean(history.history[\"accuracy\"])\n",
    "baseline_all_train_loss = np.mean(history.history[\"loss\"])\n",
    "print(\"BASELINE RESULTS:\")\n",
    "print(\"-\"*len(\"BASELINE RESULTS:\"))\n",
    "print()\n",
    "print(\"**Training**\")\n",
    "print(\"The average training accuracy among all epochs is: {:.4}\".format(baseline_all_train_acc))\n",
    "print(\"The average training loss among all epochs is: {:.4}\".format(baseline_all_train_loss))\n",
    "print()\n",
    "print(\"**Validation**\")\n",
    "print(\"The average validation accuracy among all epochs is: {:.4}\".format(baseline_all_val_acc))\n",
    "print(\"The average validation loss among all epochs is: {:.4}\".format(baseline_all_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOFuKRShbvTU"
   },
   "source": [
    "### 2.2 Analyze the performance of the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58gf79ODcFPD"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##Plot for the accuracy of the baseline model \n",
    "accuracy_train = history.history['accuracy']\n",
    "accuracy_val = history.history['val_accuracy']\n",
    "plt.plot(accuracy_train, label='training_accuracy')\n",
    "plt.plot(accuracy_val, label='validation_accuracy')\n",
    "plt.title('ACCURACY OF THE MODEL')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "##Plot for the loss of the baseline model \n",
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "plt.plot(loss_train, label='training_loss')\n",
    "plt.plot(loss_val, label='validation_loss')\n",
    "plt.title('LOSS OF MODEL')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "##ROC curve \n",
    "y_pred = baseline_model.predict(X_test_norm) \n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "#calculating roc for each class\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_onehot[:,i], y_pred[:,i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# calculating micro-average ROC curve and  area\n",
    "fpr_micro, tpr_micro, _ = roc_curve(y_test_onehot.ravel(), y_pred.ravel())\n",
    "roc_auc_micro = roc_auc_score(y_test_onehot.ravel(), y_pred.ravel())\n",
    "# Compute macro-average ROC curve and  area\n",
    "fpr_macro = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "tpr_macro = np.zeros_like(fpr_macro)\n",
    "for i in range(num_classes):\n",
    "    tpr_macro += np.interp(fpr_macro, fpr[i], tpr[i])\n",
    "tpr_macro /= num_classes\n",
    "roc_auc_macro = auc(fpr_macro, tpr_macro)\n",
    "#Plot the ROC curve for each class using matplotlib.pyplot.plot()\n",
    "plt.figure(figsize=(10, 5))\n",
    "lw = 2\n",
    "for i in range(num_classes):\n",
    "    plt.plot(fpr[i], tpr[i], lw=lw, label='ROC curve of class %d (area = %0.2f)' % (i, roc_auc[i]))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.plot(fpr_micro, tpr_micro,lw=lw, linestyle='--', label='micro-average ROC curve (area = %0.2f)' % (roc_auc_micro))\n",
    "plt.plot(fpr_macro, tpr_macro,lw=lw, linestyle='--', label='macro-average ROC curve (area = %0.2f)' % (roc_auc_macro))\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic of Multiclass')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#reversing pred to categorical so to get the labels \n",
    "inverse_label_map = {v: k for k, v in label_map.items()}  # invert the label_map\n",
    "y_pred_decoded_numerical = np.argmax(y_pred, axis=1)\n",
    "y_pred_decoded_categorical = np.vectorize(inverse_label_map.get)(y_pred_decoded_numerical)\n",
    "\n",
    "\n",
    "\n",
    "#confusion matrix \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred_decoded_categorical)\n",
    "classes = np.unique(y_test)\n",
    "# plot the confusion matrix\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap='Reds')\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=classes, yticklabels=classes, ylabel='True label', xlabel='Predicted label')\n",
    "\n",
    "# rotate the labels\n",
    "plt.setp(ax.get_xticklabels(), rotation=20, ha=\"right\", rotation_mode=\"anchor\")\n",
    "# text annotations like the numbers inside \n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#precision and recall and f1-score \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_decoded_categorical))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnxtO6BIb3_P"
   },
   "source": [
    "# 3. Adapting/fine-tuning the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#             CONTROL PANEL\n",
    "\n",
    "# Sample restriction\n",
    "restriction = True\n",
    "address_vp_imbalance = True\n",
    "proportion = 0.25\n",
    "\n",
    "# Sample augmentation\n",
    "augmentation_flip = False                       \n",
    "augmentation_rotate = False\n",
    "\n",
    "#                                           \n",
    "#############################################\n",
    "\n",
    "\n",
    "# safety mechanism: augmentation and restriction are not meant to be combined\n",
    "if restriction == True:\n",
    "    agumentation_flip = False\n",
    "    augmentation_rotate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restriction == True:\n",
    "    \n",
    "    if address_vp_imbalance == True:\n",
    "        restricted_sample_train, restricted_sample_labels_train = restrict_sample_vp_imb(proportion , X_train, y_train)\n",
    "        \n",
    "    else:\n",
    "        restricted_sample_train, restricted_sample_labels_train = restrict_sample(proportion , X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE RESTRICTION SUMMARY\n",
      "\n",
      "\n",
      "ORIGINAL SAMPLE\n",
      "Original number of instances: 4147\n",
      "Original instance distribution by class: \n",
      " Bacterial Pneumonia       1689\n",
      "Viral Pneumonia           1182\n",
      "No Pneumonia (healthy)     963\n",
      "COVID-19                   313\n",
      "dtype: int64\n",
      "\n",
      "RESTRICTED SAMPLE\n",
      "Number of instances in restricted sample: 957\n",
      "Instance distribution by class in restricted sample: \n",
      " Bacterial Pneumonia       422\n",
      "COVID-19                  313\n",
      "Viral Pneumonia           295\n",
      "No Pneumonia (healthy)    240\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if restriction == True:\n",
    "    \n",
    "    ############################\n",
    "    # > RESTRICTION  SUMMARY < #\n",
    "    ############################\n",
    "    print(\"SAMPLE RESTRICTION SUMMARY\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"ORIGINAL SAMPLE\")\n",
    "    print(F\"Original number of instances: {len(X_train)}\")\n",
    "    print(f\"Original instance distribution by class: \\n {pd.Series(y_train).value_counts()}\")\n",
    "    print(\"\")\n",
    "    print(\"RESTRICTED SAMPLE\")\n",
    "    print(f\"Number of instances in restricted sample: {len(restricted_sample)}\")\n",
    "    print(f\"Instance distribution by class in restricted sample: \\n {pd.Series(restricted_sample_labels).value_counts()}\")\n",
    "    \n",
    "    X_train = np.array(restricted_sample)\n",
    "    y_train = np.array(restricted_sample_labels)\n",
    "    \n",
    "else:\n",
    "    print(\"SAMPLE RESTRICTION WAS NOT CONDUCTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if augmentation_flip == True:\n",
    "\n",
    "    augmentedfeatures, augmentedlabels_ = augment_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE AUGMENTATION WAS NOT CONDUCTED\n"
     ]
    }
   ],
   "source": [
    "if augmentation_flip == True:\n",
    "    \n",
    "    ############################\n",
    "    # > AUGMENTATION SUMMARY < #\n",
    "    ############################\n",
    "    \n",
    "    print(\"SAMPLE AUGMENTATION SUMMARY\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"ORIGINAL SAMPLE\")\n",
    "    print(F\"Original number of instances: {len(labels)}\")\n",
    "    print(f\"Original instance distribution by class: \\n {pd.Series(labels).value_counts()}\")\n",
    "    print(\"\")\n",
    "    print(\"AUGMENTED SAMPLE\")\n",
    "    print(f\"Number of instances in augmented sample: {len(augmentedlabels)}\")\n",
    "    print(f\"Instance distribution by class in augmented sample: \\n {pd.Series(augmentedlabels).value_counts()}\")\n",
    "    \n",
    "    X_train = np.array(augmentedfeatures)\n",
    "    y_train = np.array(augmentedlabels)\n",
    "\n",
    "else:\n",
    "    print(\"SAMPLE AUGMENTATION WAS NOT CONDUCTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Categorical encoding of new labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a dictionary that maps each category to a numerical value\n",
    "#label_map = {\"Bacterial Pneumonia\": 0, \"Viral Pneumonia\": 1, \"No Pneumonia (healthy)\": 2, \"COVID-19\": 3}\n",
    "#\n",
    "## Encode the categorical labels as numerical values using the label map\n",
    "y_train_encoded = np.vectorize(label_map.get)(y_train)\n",
    "#y_val_encoded = np.vectorize(label_map.get)(y_val)\n",
    "#y_test_encoded = np.vectorize(label_map.get)(y_test)\n",
    "#\n",
    "## Convert the numerical labels to one-hot encoded format\n",
    "#num_classes = 4\n",
    "y_train_onehot = keras.utils.to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "#y_val_onehot = keras.utils.to_categorical(y_val_encoded, num_classes=num_classes)\n",
    "#y_test_onehot = keras.utils.to_categorical(y_test_encoded, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Network fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiDja0Mub4YW"
   },
   "outputs": [],
   "source": [
    "#After some tryouts, Nadam reported an extra 0-2% val_accuracy in our baseline model vs Adam\n",
    "#So we will find the best learning rate for this optimizer:\n",
    "\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def lr_schedule(epoch, initial_lr, final_lr, total_epochs):\n",
    "    \"\"\"\n",
    "    calculates the learning rate for each epoch based on the initial learning rate, final learning rate, and total number of epochs\n",
    "    \"\"\"\n",
    "    lr = initial_lr + (final_lr - initial_lr) * (epoch / float(total_epochs))\n",
    "    return lr\n",
    "\n",
    "def plot_lr_schedule(initial_lr, final_lr, total_epochs):\n",
    "    lr = [lr_schedule(epoch, initial_lr, final_lr, total_epochs) for epoch in range(total_epochs)]\n",
    "    plt.plot(lr, history.history['val_loss'])\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.show()\n",
    "\n",
    "# Adding a Lr Scheduler to check the learning rate evolution during training and to avoid overfitting\n",
    "initial_lr = 0.001\n",
    "final_lr = 0.01\n",
    "baseline_epochs = 10\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(lambda epoch: lr_schedule(epoch, initial_lr, final_lr, baseline_epochs))\n",
    "\n",
    "baseline_model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 4 epochs with a batch size of 32\n",
    "history = baseline_model.fit(\n",
    "    X_train_norm,\n",
    "    y_train_onehot,\n",
    "    batch_size=32,\n",
    "    epochs=baseline_epochs,\n",
    "    validation_data=(X_val_norm, y_val_onehot),\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule of thumb: optimal will be a bit lower than when lr starts climbing, usually 10 times lower the climb up point (around 0.005)\n",
    "plot_lr_schedule(initial_lr, final_lr, baseline_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from keras import backend as K\n",
    "\n",
    "# Some memory clean-up\n",
    "K.clear_session()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-05 12:15:56.749145: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-05 12:15:59.214801: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-05 12:15:59.443697: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-05 12:15:59.443875: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-05 12:16:07.024070: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-05 12:16:07.025767: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-05 12:16:07.025799: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# TUNED MODEL 1: BASELINE + LR: Nadam + KFOLD VALIDATION + EARLY STOPPING + LeakyReLU + Extra Dense layer + 3 Dropout layers + Doubled batch size to 64\n",
    "# FAILED CHANGES VS BASELINE INDICATED WITH A HASHTAG\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "tuned_m_batch = 64\n",
    "\n",
    "def build_tuned_model():\n",
    "    model = keras.Sequential([\n",
    "    # Convolutional layers\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=LeakyReLU(), input_shape=(156, 156, 3)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=LeakyReLU()),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=LeakyReLU()),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=LeakyReLU()),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=LeakyReLU()),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=LeakyReLU()),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    # (FAILED CHANGE VS BASELINE) Adding another pack of Conv2D and MaxPooling2D layers\n",
    "    #keras.layers.Conv2D(filters=64, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.01)),\n",
    "    #keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.01)),\n",
    "    #keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    # Dense layers\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(32, activation=LeakyReLU()),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(32, activation=LeakyReLU()),\n",
    "    keras.layers.Dropout(0.2),    \n",
    "    keras.layers.Dense(32, activation=LeakyReLU()),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(4, activation='softmax')])\n",
    "    # Compile the model with appropriate loss function, optimizer, and metrics\n",
    "    # (FAILED CHANGE VS BASELINE) Adding the optimal using lr schedule for SGD\n",
    "    optim = keras.optimizers.Nadam(learning_rate=0.001) #(FAILED CHANGE VS BASELINE) optimizer = adam, sgd, rmsprop\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_h_model():\n",
    "    '''restnet blocks'''\n",
    "    input_shape = (156, 156, 3)\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "     # Convolutional layers\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.01),  padding='same')(inputs)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.01), padding='same')(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    #  block 1\n",
    "    out=x\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.add([out, x])\n",
    "    x = keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    #  block 2\n",
    "    out=x\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.add([out, x])\n",
    "    x = keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    #  block 3\n",
    "    out=x\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.add([out, x])\n",
    "    x = keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    # Dense layers\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(32, activation=keras.layers.LeakyReLU(alpha=0.1))(x)\n",
    "    x = keras.layers.Dense(32, activation=keras.layers.LeakyReLU(alpha=0.1))(x)\n",
    "    outputs = keras.layers.Dense(4, activation='softmax')(x)\n",
    "    optim = keras.optimizers.Nadam(learning_rate=0.001) \n",
    "\n",
    "\n",
    "    # Create model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='h_model')\n",
    "\n",
    "    # Compile the model with appropriate loss function, optimizer, and metrics\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "hybrid_model = build_h_model()\n",
    "\n",
    "# Train the model for 10 epochs with a batch size of 32\n",
    "'''history = hybrid_model.fit(\n",
    "    X_train_norm,\n",
    "    y_train_onehot,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_norm, y_val_onehot))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K FOLD VALIDATION (5 max epochs for speeding purposes)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k = 4\n",
    "num_val_samples = len(X_train_norm) // k \n",
    "num_epochs = 5\n",
    "tuned_all_val_losses = [] # Should add the score of each run at the end of the loop\n",
    "tuned_all_val_acc = []\n",
    "base_all_val_losses = []\n",
    "base_all_val_acc = []\n",
    "res_all_val_acc=[]\n",
    "res_all_val_losses=[]\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = X_train_norm[i * num_val_samples: (i + 1) * num_val_samples] \n",
    "    val_targets = y_train_onehot[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [X_train_norm[:i * num_val_samples],\n",
    "         X_train_norm[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [y_train_onehot[:i * num_val_samples],\n",
    "         y_train_onehot[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    # (CHANGE VS BASELINE)Defining EarlyStopping callback\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "    baseline_model = build_baseline_model()\n",
    "    tuned_model_1 = build_tuned_model()\n",
    "    hybrid_model = build_h_model()\n",
    "    \n",
    "\n",
    "    # Train baseline and tuned models for 4 epochs with a batch size of 32\n",
    "    print('processing baseline model')\n",
    "    baseline_history = baseline_model.fit(\n",
    "        partial_train_data,\n",
    "        partial_train_targets,\n",
    "        batch_size=32,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(val_data, val_targets),\n",
    "    )\n",
    "    print('processing tuned model')\n",
    "    tuned_history = tuned_model_1.fit(\n",
    "        partial_train_data,\n",
    "        partial_train_targets,\n",
    "        batch_size=tuned_m_batch,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(val_data, val_targets),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    print('processing hibrid model 1')\n",
    "    h_history= hybrid_model.fit(\n",
    "        partial_train_data,\n",
    "        partial_train_targets,\n",
    "        batch_size=32,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(val_data, val_targets),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    # Evaluate the kfold results for BASELINE\n",
    "    base_val_loss, base_val_accuracy = baseline_model.evaluate(val_data, val_targets, verbose=0)\n",
    "    base_all_val_losses.append(base_val_loss)\n",
    "    base_all_val_acc.append(base_val_accuracy)\n",
    "\n",
    "    # Evaluate the kfold results for the tuned model\n",
    "    tuned_val_loss, tuned_val_accuracy = tuned_model_1.evaluate(val_data, val_targets, verbose=0)\n",
    "    tuned_all_val_losses.append(tuned_val_loss)\n",
    "    tuned_all_val_acc.append(tuned_val_accuracy)\n",
    "\n",
    "    # Evaluate the kfold results for the hibrid model 1\n",
    "    res_val_loss, res_val_accuracy = hybrid_model.evaluate(val_data, val_targets, verbose=0)\n",
    "    res_all_val_losses.append(res_val_loss)\n",
    "    res_all_val_acc.append(res_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline Model KF Results:\")\n",
    "print(\"-\"*len(\"Baseline Model Results:\"))\n",
    "print(\"Avg val_acc: {}\".format(np.mean(base_all_val_acc)))\n",
    "print(\"Avg val_loss: {}\".format(np.mean(base_all_val_losses)))\n",
    "print()\n",
    "print(\"Tuned Model KF Results:\")\n",
    "print(\"-\"*len(\"Tuned Model KF Results:\"))\n",
    "print(\"Avg val_acc: {}\".format(np.mean(tuned_all_val_acc)))\n",
    "print(\"Avg val_loss: {}\".format(np.mean(tuned_all_val_losses)))\n",
    "print()\n",
    "print(\"ResNet Model KF Results:\")\n",
    "print(\"-\"*len(\"ResNetmodel KF Results:\"))\n",
    "print(\"Avg val_acc: {}\".format(np.mean(res_all_val_acc)))\n",
    "print(\"Avg val_loss: {}\".format(np.mean(res_all_val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, num_epochs+1)\n",
    "\n",
    "# Plotting the results of validation accuracy and loss for the baseline and tuned models\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, baseline_history.history['val_accuracy'], 'b', label='Baseline model')\n",
    "plt.plot(epochs, tuned_history.history['val_accuracy'], 'r', label='Tuned model')\n",
    "plt.title('Validation accuracy comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, baseline_history.history['val_loss'], 'b', label='Baseline model')\n",
    "plt.plot(epochs, tuned_history.history['val_loss'], 'r', label='Tuned model')\n",
    "plt.title('Validation loss comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the results of training accuracy and loss for the baseline and tuned models  \n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, baseline_history.history['accuracy'], 'b', label='Baseline model')\n",
    "plt.plot(epochs, tuned_history.history['accuracy'], 'r', label='Tuned model')\n",
    "plt.title('Training accuracy comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epochs, baseline_history.history['loss'], 'b', label='Baseline model')\n",
    "plt.plot(epochs, tuned_history.history['loss'], 'r', label='Tuned model')\n",
    "plt.title('Training loss comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Analyze performance of fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBGr0x8ZcFn9"
   },
   "source": [
    "# 4. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fn_WnCfDcFyD"
   },
   "source": [
    "### 4.1 New data split adapted to transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Transfer learning with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VGG16\n",
    "vgg_model = VGG16(include_top=False, input_shape=(156, 156, 3))\n",
    "\n",
    "# FReezing VGG16 layers\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "## adding \"custom\" layers\n",
    "\n",
    "## Flatten layer\n",
    "flat_1 = layers.Flatten()(vgg_model.layers[-1].output)\n",
    "\n",
    "## Dense layers\n",
    "dense_1 = layers.Dense(32, activation='relu')(flat_1)\n",
    "\n",
    "#output layer with softmax \n",
    "output = layers.Dense(4, activation='softmax')(dense_1)\n",
    "\n",
    "# define new model\n",
    "tl_model = Model(inputs=vgg_model.inputs, outputs=output)\n",
    "\n",
    "# summarize\n",
    "tl_model.summary()\n",
    " \n",
    "# compile model\n",
    "tl_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)\n",
    "\n",
    "# fit model\n",
    "fitting = tl_model.fit(X_train, y_train_onehot, validation_data=(X_val, y_val_onehot), batch_size=64 ,epochs=25, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Analyze performance of transfer learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "160d3d50f83ed3d75d0672b6b4d1134bc09c3e08300a8645f571fa0ede095231"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
