{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xveQIshNWYdc"
   },
   "source": [
    "# Loading Dataset\n",
    "\n",
    "You can download the dataset from {https://darwin.v7labs.com/v7-labs/covid-19-chest-x-ray-dataset?sort=priority\\%3Adesc}.\n",
    "The data entitled as '`darwin dataset pull v7-labs/covid-19-chest-x-ray-dataset:all-images`' will be used in this assignment. All dataset consist of 6504 images from 702 classes. We will extract the images of 4 classes (Bacterial Pneumonia, Viral Pneumonia, No Pneumonia (healthy), Covid-19) and save them as .npy file with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6bqTVRSs-sw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import keras\n",
    "import json\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "from keras import layers, models\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nContinue working in next cell\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################\n",
    "# > DISABLED - ALREADY EXECUTED < #\n",
    "###################################\n",
    "\n",
    "'''\n",
    "Continue working in next cell\n",
    "'''\n",
    "\n",
    "# # all-images file should be uploaded to the same file\n",
    "# imageNames = glob.glob(\"all-images/*\")\n",
    "# \n",
    "# dataset = []\n",
    "# labels = []\n",
    "# \n",
    "# for i, imName in enumerate(imageNames):\n",
    "# \n",
    "#     # Opening JSON file\n",
    "#     f = open(imName)\n",
    "#     data = json.load(f)\n",
    "#     for j in range(len(data['annotations'])):\n",
    "# \n",
    "#         if 'COVID-19' in (data['annotations'][j]['name']):\n",
    "#           #load images from url    \n",
    "#             urllib.request.urlretrieve(data['image']['url'],\"img.png\")    \n",
    "#             img = Image.open(\"img.png\")\n",
    "#             #convert images to grayscale\n",
    "#             imgGray = img.convert('L')\n",
    "#             #resize the image (156x156)\n",
    "#             im = imgGray.resize((156,156), Image.LANCZOS)           \n",
    "#             label = data['annotations'][j]['name']\n",
    "#             dataset.append(np.array(im))\n",
    "#             labels.append(label)\n",
    "#             print(label)\n",
    "#             break\n",
    "# \n",
    "#         if 'Viral Pneumonia' in (data['annotations'][j]['name']) \\\n",
    "#             or 'Bacterial Pneumonia' in (data['annotations'][j]['name']) \\\n",
    "#             or 'No Pneumonia (healthy)' in (data['annotations'][j]['name']):\n",
    "#             #load images from url    \n",
    "#             urllib.request.urlretrieve(data['image']['url'],\"img.png\")    \n",
    "#             img = Image.open(\"img.png\")\n",
    "#             #convert images to grayscale\n",
    "#             imgGray = img.convert('L')\n",
    "#             #resize the image (156x156)\n",
    "#             im = imgGray.resize((156,156), Image.LANCZOS)           \n",
    "#             label = data['annotations'][j]['name']\n",
    "#             dataset.append(np.array(im))\n",
    "#             labels.append(label)\n",
    "#             break\n",
    "# \n",
    "# #Convert data shape of (n_of_samples, width, height, 1)\n",
    "# dataset = np.dstack(dataset)    \n",
    "# dataset = np.rollaxis(dataset,-1)\n",
    "# labels = np.array(labels)\n",
    "# \n",
    "# #convert images gray scale to rgb\n",
    "# data = np.array(layers.Lambda(tf.image.grayscale_to_rgb)(tf.expand_dims(dataset, -1)))\n",
    "# \n",
    "# # save data and labels into a folder\n",
    "# np.save(\"data.npy\", data)\n",
    "# np.save(\"labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn3sH7yJWboe"
   },
   "source": [
    "Once you save your data, you can load it from your directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "b6ZPDhVLWbyq"
   },
   "outputs": [],
   "source": [
    "data = np.load('data.npy')\n",
    "labels = np.load('labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1b7dP-VbJpC"
   },
   "source": [
    "# Preprocessing Steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xC_YmdQMbn1K"
   },
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xStbdJAHblFv"
   },
   "outputs": [],
   "source": [
    "# stratify?\n",
    "# shuffle?\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "# Replace 'features' and 'labels' with your actual feature and label data\n",
    "features = np.load('data.npy')\n",
    "labels = np.load('labels.npy')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Further split the training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-r4ta3MbKLJ"
   },
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LeplL77mbKS4"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Compute the mean and standard deviation of the training set\n",
    "train_mean = np.mean(X_train, axis=0)\n",
    "train_std = np.std(X_train, axis=0)\n",
    "\n",
    "# Normalize each set separately using the training set statistics\n",
    "X_train_norm = (X_train - train_mean) / train_std\n",
    "X_val_norm = (X_val - train_mean) / train_std\n",
    "X_test_norm = (X_test - train_mean) / train_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary that maps each category to a numerical value\n",
    "label_map = {\"Bacterial Pneumonia\": 0, \"Viral Pneumonia\": 1, \"No Pneumonia (healthy)\": 2, \"COVID-19\": 3}\n",
    "\n",
    "# Encode the categorical labels as numerical values using the label map\n",
    "y_train_encoded = np.vectorize(label_map.get)(y_train)\n",
    "y_val_encoded = np.vectorize(label_map.get)(y_val)\n",
    "y_test_encoded = np.vectorize(label_map.get)(y_test)\n",
    "\n",
    "# Convert the numerical labels to one-hot encoded format\n",
    "num_classes = 4\n",
    "y_train_onehot = keras.utils.to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_val_onehot = keras.utils.to_categorical(y_val_encoded, num_classes=num_classes)\n",
    "y_test_onehot = keras.utils.to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YONVgtOAbKca"
   },
   "source": [
    "# Create Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "LhUlV9UNbKiH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "128/128 [==============================] - 645s 5s/step - loss: 0.7968 - accuracy: 0.6746 - val_loss: 0.6137 - val_accuracy: 0.7390\n",
      "Epoch 2/10\n",
      "128/128 [==============================] - 536s 4s/step - loss: 0.6372 - accuracy: 0.7391 - val_loss: 0.6423 - val_accuracy: 0.7312\n",
      "Epoch 3/10\n",
      "128/128 [==============================] - 473s 4s/step - loss: 0.5594 - accuracy: 0.7702 - val_loss: 0.5811 - val_accuracy: 0.7586\n",
      "Epoch 4/10\n",
      "128/128 [==============================] - 373s 3s/step - loss: 0.5050 - accuracy: 0.7939 - val_loss: 0.5608 - val_accuracy: 0.7615\n",
      "Epoch 5/10\n",
      "128/128 [==============================] - 374s 3s/step - loss: 0.4573 - accuracy: 0.8144 - val_loss: 0.6046 - val_accuracy: 0.7576\n",
      "Epoch 6/10\n",
      "128/128 [==============================] - 369s 3s/step - loss: 0.4232 - accuracy: 0.8262 - val_loss: 0.5628 - val_accuracy: 0.7889\n",
      "Epoch 7/10\n",
      "128/128 [==============================] - 359s 3s/step - loss: 0.3493 - accuracy: 0.8567 - val_loss: 0.5934 - val_accuracy: 0.7634\n",
      "Epoch 8/10\n",
      "128/128 [==============================] - 399s 3s/step - loss: 0.2887 - accuracy: 0.8804 - val_loss: 0.7404 - val_accuracy: 0.7693\n",
      "Epoch 9/10\n",
      "128/128 [==============================] - 385s 3s/step - loss: 0.2571 - accuracy: 0.8978 - val_loss: 0.7557 - val_accuracy: 0.7468\n",
      "Epoch 10/10\n",
      "128/128 [==============================] - 385s 3s/step - loss: 0.1821 - accuracy: 0.9306 - val_loss: 0.9375 - val_accuracy: 0.7498\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # Convolutional layers\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(156, 156, 3)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Dense layers\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with appropriate loss function, optimizer, and metrics\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 10 epochs with a batch size of 32\n",
    "history = model.fit(\n",
    "    X_train_norm,\n",
    "    y_train_onehot,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_norm, y_val_onehot)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOFuKRShbvTU"
   },
   "source": [
    "# Analyze the performance of the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58gf79ODcFPD"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy_train = history.history['accuracy']\n",
    "accuracy_val = history.history['val_accuracy']\n",
    "plt.plot(accuracy_train, label='training_accuracy')\n",
    "plt.plot(accuracy_val, label='validation_accuracy')\n",
    "plt.title('ACCURACY OF THE MODEL')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnxtO6BIb3_P"
   },
   "source": [
    "# Adapting/fine-tuning the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiDja0Mub4YW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBGr0x8ZcFn9"
   },
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fn_WnCfDcFyD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "160d3d50f83ed3d75d0672b6b4d1134bc09c3e08300a8645f571fa0ede095231"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
