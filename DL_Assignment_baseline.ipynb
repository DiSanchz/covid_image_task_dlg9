{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xveQIshNWYdc"
   },
   "source": [
    "# 0. Loading Dataset\n",
    "\n",
    "You can download the dataset from {https://darwin.v7labs.com/v7-labs/covid-19-chest-x-ray-dataset?sort=priority\\%3Adesc}.\n",
    "The data entitled as '`darwin dataset pull v7-labs/covid-19-chest-x-ray-dataset:all-images`' will be used in this assignment. All dataset consist of 6504 images from 702 classes. We will extract the images of 4 classes (Bacterial Pneumonia, Viral Pneumonia, No Pneumonia (healthy), Covid-19) and save them as .npy file with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I6bqTVRSs-sw",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 06:23:27.401136: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 06:23:28.609245: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-04 06:23:28.609312: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-04 06:23:32.272777: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 06:23:32.273523: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 06:23:32.273543: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import keras\n",
    "import json\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "from keras import layers, models\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from skimage import io, color\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# > DISABLED - ALREADY EXECUTED < #\n",
    "###################################\n",
    "\n",
    "\n",
    "# # all-images file should be uploaded to the same file\n",
    "# imageNames = glob.glob(\"all-images/*\")\n",
    "# \n",
    "# dataset = []\n",
    "# labels = []\n",
    "# \n",
    "# for i, imName in enumerate(imageNames):\n",
    "# \n",
    "#     # Opening JSON file\n",
    "#     f = open(imName)\n",
    "#     data = json.load(f)\n",
    "#     for j in range(len(data['annotations'])):\n",
    "# \n",
    "#         if 'COVID-19' in (data['annotations'][j]['name']):\n",
    "#           #load images from url    \n",
    "#             urllib.request.urlretrieve(data['image']['url'],\"img.png\")    \n",
    "#             img = Image.open(\"img.png\")\n",
    "#             #convert images to grayscale\n",
    "#             imgGray = img.convert('L')\n",
    "#             #resize the image (156x156)\n",
    "#             im = imgGray.resize((156,156), Image.LANCZOS)           \n",
    "#             label = data['annotations'][j]['name']\n",
    "#             dataset.append(np.array(im))\n",
    "#             labels.append(label)\n",
    "#             print(label)\n",
    "#             break\n",
    "# \n",
    "#         if 'Viral Pneumonia' in (data['annotations'][j]['name']) \\\n",
    "#             or 'Bacterial Pneumonia' in (data['annotations'][j]['name']) \\\n",
    "#             or 'No Pneumonia (healthy)' in (data['annotations'][j]['name']):\n",
    "#             #load images from url    \n",
    "#             urllib.request.urlretrieve(data['image']['url'],\"img.png\")    \n",
    "#             img = Image.open(\"img.png\")\n",
    "#             #convert images to grayscale\n",
    "#             imgGray = img.convert('L')\n",
    "#             #resize the image (156x156)\n",
    "#             im = imgGray.resize((156,156), Image.LANCZOS)           \n",
    "#             label = data['annotations'][j]['name']\n",
    "#             dataset.append(np.array(im))\n",
    "#             labels.append(label)\n",
    "#             break\n",
    "# \n",
    "# #Convert data shape of (n_of_samples, width, height, 1)\n",
    "# dataset = np.dstack(dataset)    \n",
    "# dataset = np.rollaxis(dataset,-1)\n",
    "# labels = np.array(labels)\n",
    "# \n",
    "# #convert images gray scale to rgb\n",
    "# data = np.array(layers.Lambda(tf.image.grayscale_to_rgb)(tf.expand_dims(dataset, -1)))\n",
    "# \n",
    "# # save data and labels into a folder\n",
    "# np.save(\"data.npy\", data)\n",
    "# np.save(\"labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn3sH7yJWboe"
   },
   "source": [
    "Once you save your data, you can load it from your directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b6ZPDhVLWbyq"
   },
   "outputs": [],
   "source": [
    "features = np.load('data/data.npy')\n",
    "labels = np.load('data/labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1b7dP-VbJpC"
   },
   "source": [
    "# 1. Data preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note from Diego: discuss in the meeting \n",
    "what is exactly stratify and do we need it?\n",
    "'''\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Further split the training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-r4ta3MbKLJ"
   },
   "source": [
    "### 1.2 Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LeplL77mbKS4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Note from Diego:  discuss in the meeting \n",
    "shall we keep only one code snippet for normalize? Select whichever is the most appropriate\n",
    "'''\n",
    "\n",
    "#making them float \n",
    "X_train=X_train.astype('float32')\n",
    "X_test=X_test.astype('float32')\n",
    "X_val=X_val.astype('float32')\n",
    "\n",
    "#Normalizing the data between 0 and 1 \n",
    "X_train=X_train/255.0\n",
    "X_test=X_test/255.0\n",
    "X_val=X_val/255.0\n",
    "\n",
    "\n",
    "# Compute the mean and standard deviation of the training set\n",
    "train_mean = np.mean(X_train, axis=0)\n",
    "train_std = np.std(X_train, axis=0)\n",
    "\n",
    "# Normalize each set separately using the training set statistics\n",
    "X_train_norm = (X_train - train_mean) / train_std\n",
    "X_val_norm = (X_val - train_mean) / train_std\n",
    "X_test_norm = (X_test - train_mean) / train_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary that maps each category to a numerical value\n",
    "label_map = {\"Bacterial Pneumonia\": 0, \"Viral Pneumonia\": 1, \"No Pneumonia (healthy)\": 2, \"COVID-19\": 3}\n",
    "\n",
    "# Encode the categorical labels as numerical values using the label map\n",
    "y_train_encoded = np.vectorize(label_map.get)(y_train)\n",
    "y_val_encoded = np.vectorize(label_map.get)(y_val)\n",
    "y_test_encoded = np.vectorize(label_map.get)(y_test)\n",
    "\n",
    "# Convert the numerical labels to one-hot encoded format\n",
    "num_classes = 4\n",
    "y_train_onehot = keras.utils.to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_val_onehot = keras.utils.to_categorical(y_val_encoded, num_classes=num_classes)\n",
    "y_test_onehot = keras.utils.to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YONVgtOAbKca"
   },
   "source": [
    "# 2. Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "LhUlV9UNbKiH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "128/128 [==============================] - 645s 5s/step - loss: 0.7968 - accuracy: 0.6746 - val_loss: 0.6137 - val_accuracy: 0.7390\n",
      "Epoch 2/10\n",
      "128/128 [==============================] - 536s 4s/step - loss: 0.6372 - accuracy: 0.7391 - val_loss: 0.6423 - val_accuracy: 0.7312\n",
      "Epoch 3/10\n",
      "128/128 [==============================] - 473s 4s/step - loss: 0.5594 - accuracy: 0.7702 - val_loss: 0.5811 - val_accuracy: 0.7586\n",
      "Epoch 4/10\n",
      "128/128 [==============================] - 373s 3s/step - loss: 0.5050 - accuracy: 0.7939 - val_loss: 0.5608 - val_accuracy: 0.7615\n",
      "Epoch 5/10\n",
      "128/128 [==============================] - 374s 3s/step - loss: 0.4573 - accuracy: 0.8144 - val_loss: 0.6046 - val_accuracy: 0.7576\n",
      "Epoch 6/10\n",
      "128/128 [==============================] - 369s 3s/step - loss: 0.4232 - accuracy: 0.8262 - val_loss: 0.5628 - val_accuracy: 0.7889\n",
      "Epoch 7/10\n",
      "128/128 [==============================] - 359s 3s/step - loss: 0.3493 - accuracy: 0.8567 - val_loss: 0.5934 - val_accuracy: 0.7634\n",
      "Epoch 8/10\n",
      "128/128 [==============================] - 399s 3s/step - loss: 0.2887 - accuracy: 0.8804 - val_loss: 0.7404 - val_accuracy: 0.7693\n",
      "Epoch 9/10\n",
      "128/128 [==============================] - 385s 3s/step - loss: 0.2571 - accuracy: 0.8978 - val_loss: 0.7557 - val_accuracy: 0.7468\n",
      "Epoch 10/10\n",
      "128/128 [==============================] - 385s 3s/step - loss: 0.1821 - accuracy: 0.9306 - val_loss: 0.9375 - val_accuracy: 0.7498\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "def build_baseline_model():\n",
    "    model = keras.Sequential([\n",
    "        # Convolutional layers\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(156, 156, 3)),\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Dense layers\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model with appropriate loss function, optimizer, and metrics\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = build_baseline_model()\n",
    "\n",
    "# Train the model for 10 epochs with a batch size of 32\n",
    "history = baseline_model.fit(\n",
    "    X_train_norm,\n",
    "    y_train_onehot,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_norm, y_val_onehot)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOFuKRShbvTU"
   },
   "source": [
    "### 2.2 Analyze the performance of the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58gf79ODcFPD"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##Plot for the accuracy of the baseline model \n",
    "accuracy_train = history.history['accuracy']\n",
    "accuracy_val = history.history['val_accuracy']\n",
    "plt.plot(accuracy_train, label='training_accuracy')\n",
    "plt.plot(accuracy_val, label='validation_accuracy')\n",
    "plt.title('ACCURACY OF THE MODEL')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "##Plot for the loss of the baseline model \n",
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "plt.plot(loss_train, label='training_accuracy')\n",
    "plt.plot(loss_val, label='validation_accuracy')\n",
    "plt.title('LOSS OF MODEL')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "##ROC curve \n",
    "y_pred = model.predict(X_test_norm) \n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "#calculating roc for each class\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_onehot[:,i], y_pred[:,i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# calculating micro-average ROC curve and  area\n",
    "fpr_micro, tpr_micro, _ = roc_curve(y_test_onehot.ravel(), y_pred.ravel())\n",
    "roc_auc_micro = roc_auc_score(y_test_onehot.ravel(), y_pred.ravel())\n",
    "# Compute macro-average ROC curve and  area\n",
    "fpr_macro = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "tpr_macro = np.zeros_like(fpr_macro)\n",
    "for i in range(num_classes):\n",
    "    tpr_macro += np.interp(fpr_macro, fpr[i], tpr[i])\n",
    "tpr_macro /= num_classes\n",
    "roc_auc_macro = auc(fpr_macro, tpr_macro)\n",
    "#Plot the ROC curve for each class using matplotlib.pyplot.plot()\n",
    "plt.figure(figsize=(10, 5))\n",
    "lw = 2\n",
    "for i in range(num_classes):\n",
    "    plt.plot(fpr[i], tpr[i], lw=lw, label='ROC curve of class %d (area = %0.2f)' % (i, roc_auc[i]))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.plot(fpr_micro, tpr_micro,lw=lw, linestyle='--', label='micro-average ROC curve (area = %0.2f)' % (roc_auc_micro))\n",
    "plt.plot(fpr_macro, tpr_macro,lw=lw, linestyle='--', label='macro-average ROC curve (area = %0.2f)' % (roc_auc_macro))\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic of Multiclass')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#reversing pred to categorical so to get the labels \n",
    "inverse_label_map = {v: k for k, v in label_map.items()}  # invert the label_map\n",
    "y_pred_decoded_numerical = np.argmax(y_pred, axis=1)\n",
    "y_pred_decoded_categorical = np.vectorize(inverse_label_map.get)(y_pred_decoded_numerical)\n",
    "\n",
    "\n",
    "\n",
    "#confusion matrix \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred_decoded_categorical)\n",
    "classes = np.unique(y_test)\n",
    "# plot the confusion matrix\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap='Reds')\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=classes, yticklabels=classes, ylabel='True label', xlabel='Predicted label')\n",
    "\n",
    "# rotate the labels\n",
    "plt.setp(ax.get_xticklabels(), rotation=20, ha=\"right\", rotation_mode=\"anchor\")\n",
    "# text annotations like the numbers inside \n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#precision and recall and f1-score \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_decoded_categorical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnxtO6BIb3_P"
   },
   "source": [
    "# 3. Adapting/fine-tuning the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#             CONTROL PANEL\n",
    "\n",
    "# Sample restriction\n",
    "restriction = True\n",
    "proportion = 0.25\n",
    "\n",
    "# Sample augmentation\n",
    "augmentation_flip = False                       \n",
    "augmentation_rotate = False\n",
    "\n",
    "#                                           \n",
    "#############################################\n",
    "\n",
    "\n",
    "# safety mechanism: augmentation and restriction are not meant to be combined\n",
    "if restriction == True:\n",
    "    agumentation_flip = False\n",
    "    augmentation_rotate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some useful functions for restrition/augmentation\n",
    "\n",
    "def collect_labels(data, labels):\n",
    "    \n",
    "    img_label_dict = {}\n",
    "    \n",
    "    # relate image and label index in dictionary\n",
    "    for i in range(len(labels)):\n",
    "        img_label_dict[i] = labels[i]\n",
    "        \n",
    "        \n",
    "    # Store all the indexes by label\n",
    "    BP_index = {k for k,v in img_label_dict.items() if v == 'Bacterial Pneumonia'}\n",
    "    VP_index = {k for k,v in img_label_dict.items() if v == 'Viral Pneumonia'}\n",
    "    NP_index = {k for k,v in img_label_dict.items() if v == 'No Pneumonia (healthy)'}\n",
    "    \n",
    "    CV_index = {k for k,v in img_label_dict.items() if v == 'COVID-19'}\n",
    "\n",
    "    return [BP_index, VP_index, NP_index, CV_index], img_label_dict\n",
    "\n",
    "\n",
    "def restrict_sample(proportion, data, labels):\n",
    "\n",
    "    classified_instances, img_label_dict = collect_labels(data, labels)\n",
    "\n",
    "    restricted_sample_indices = set()\n",
    "    restricted_img_label_dict = {}\n",
    "\n",
    "    for i in classified_instances[:-1]:\n",
    "        tmp = random.sample(i, k=int(len(i)*proportion))\n",
    "        for e in tmp:\n",
    "            restricted_sample_indices.add(e)\n",
    "\n",
    "    restricted_sample_indices = restricted_sample_indices.union(classified_instances[-1])\n",
    "    restricted_sample_labels = [img_label_dict.get(i) for i in restricted_sample_indices]\n",
    "    restricted_sample = [X_train[i] for i in restricted_sample_indices]\n",
    "\n",
    "    return restricted_sample, restricted_sample_labels\n",
    "    \n",
    "def rotate90(image):\n",
    "    return tf.image.rot90(image)\n",
    "\n",
    "def rotate180(image):\n",
    "    return tf.image.rot90(rotate90(image))\n",
    "\n",
    "def rotate270(image):\n",
    "    return tf.image.rot90(rotate180(image))  \n",
    "\n",
    "\n",
    "def augment_sample(features, labels):\n",
    "\n",
    "    img_label_dict = {}\n",
    "    \n",
    "    # relate image and label index in dictionary\n",
    "    for i in range(len(labels)):\n",
    "        img_label_dict[i] = labels[i]\n",
    "        \n",
    "        \n",
    "    # Store all the indexes by label\n",
    "    BP_index = {k for k,v in img_label_dict.items() if v == 'Bacterial Pneumonia'}\n",
    "    VP_index = {k for k,v in img_label_dict.items() if v == 'Viral Pneumonia'}\n",
    "    NP_index = {k for k,v in img_label_dict.items() if v == 'No Pneumonia (healthy)'}\n",
    "    \n",
    "    CV_index = {k for k,v in img_label_dict.items() if v == 'COVID-19'}\n",
    "    \n",
    "    # Create list with all the previous lists so it can be used to iterate (except covid)\n",
    "    labels_collect = [BP_index, VP_index, NP_index]\n",
    "    \n",
    "    # Select random sample of 50% of each element which is not covid to apply flip transformation\n",
    "    # The remaining 50% stays the same\n",
    "    to_transform = set()\n",
    "    \n",
    "    for i in labels_collect:\n",
    "        tmp = random.sample(i, k=int(len(i)*0.5))\n",
    "        for e in tmp:\n",
    "            to_transform.add(e)\n",
    "            \n",
    "    not_to_transform = set(img_label_dict.keys()) - to_transform - CV_index\n",
    "    # Creating list for keeping track of the labels for the new data        \n",
    "    new_labels = []\n",
    "\n",
    "    for i in to_transform:\n",
    "        new_labels.append(img_label_dict.get(i))\n",
    "    for i in not_to_transform:\n",
    "        new_labels.append(img_label_dict.get(i))\n",
    "    \n",
    "    # Transform the sampled images and include these in the dataset while deleting the original ones\n",
    "    new_features_no_cv = []\n",
    "    \n",
    "    for i in to_transform: # First all transformed instaces are added to the new list\n",
    "            new_features_no_cv.append(tf.image.flip_left_right(features[i]))\n",
    "    for i in not_to_transform: # And then those that will stay the same\n",
    "            new_features_no_cv.append(features[i])\n",
    "    \n",
    "    # Include in new feature set covid x-rays + augmented covid x-rays\n",
    "    new_features_cv = []\n",
    "    \n",
    "    for i in CV_index:\n",
    "        new_features_cv.append(tf.image.flip_left_right(features[i]))\n",
    "        new_features_cv.append(features[i])\n",
    "    \n",
    "    if augmentation_rotate == False:\n",
    "        \n",
    "        new_labels_cv = ['COVID-19' for i in range(len(new_features_cv))]\n",
    "        \n",
    "        augmentedfeatures = new_features_no_cv + new_features_cv\n",
    "        augmentedlabels = new_labels + new_labels_cv\n",
    "        \n",
    "        return augmentedfeatures, augmentedlabels\n",
    "        \n",
    "    # AUGMENTATION - ROTATE    \n",
    "    elif augmentation_rotate == True:\n",
    "        \n",
    "        #Divide no_cv instances in 3 groups (30%, 35%, 35%)\n",
    "        \n",
    "        fd_features, features_1, fd_labels, labels_1 = train_test_split(new_features_no_cv, new_labels, \n",
    "                                                            test_size=0.3, random_state=42)\n",
    "        \n",
    "        features_2, features_3, labels_2, labels_3 = train_test_split(fd_features, fd_labels, \n",
    "                                                            test_size=0.5, random_state=42)\n",
    "        \n",
    "        # Apply rotate90 to the first group and add both original and rotated to the new feature set alongside the labels\n",
    "        \n",
    "        new_features_2 = []\n",
    "        new_labels_2 = []\n",
    "        \n",
    "        for i in range(len(features_1)):\n",
    "            new_features_2.append(features_1[i])\n",
    "            new_features_2.append(rotate90(features_1[i]))\n",
    "            for e in range(2):\n",
    "                new_labels_2.append(labels_1[i])\n",
    "        \n",
    "        # Idem 180\n",
    "        \n",
    "        for i in range(len(features_2)):\n",
    "            new_features_2.append(features_2[i])\n",
    "            new_features_2.append(rotate180(features_2[i]))\n",
    "            for e in range(2):\n",
    "                new_labels_2.append(labels_2[i])\n",
    "        \n",
    "        # Idem 270\n",
    "        \n",
    "        for i in range(len(features_3)):\n",
    "            new_features_2.append(features_3[i])\n",
    "            new_features_2.append(rotate270(features_3[i]))\n",
    "            for e in range(2):\n",
    "                new_labels_2.append(labels_3[i])\n",
    "        \n",
    "        #Apply all rotations to all cv features and add them to the new set alongside the originals and the labels\n",
    "        \n",
    "        for i in range(len(new_features_cv)):\n",
    "            new_features_2.append(new_features_cv[i])\n",
    "            new_features_2.append(rotate90(features_3[i]))\n",
    "            new_features_2.append(rotate180(features_3[i]))\n",
    "            new_features_2.append(rotate270(features_3[i]))\n",
    "            for e in range(4):\n",
    "                new_labels_2.append('COVID-19')\n",
    "        \n",
    "        \n",
    "        augmentedfeatures = new_features_2\n",
    "        augmentedlabels = new_labels_2\n",
    "        \n",
    "        return augmentedfeatures, augmentedlabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restriction == True:\n",
    "\n",
    "    restricted_sample, restricted_sample_labels = restrict_sample(proportion , X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE RESTRICTION SUMMARY\n",
      "\n",
      "\n",
      "ORIGINAL SAMPLE\n",
      "Original number of instances: 4147\n",
      "Original instance distribution by class: \n",
      " Bacterial Pneumonia       1689\n",
      "Viral Pneumonia           1182\n",
      "No Pneumonia (healthy)     963\n",
      "COVID-19                   313\n",
      "dtype: int64\n",
      "\n",
      "RESTRICTED SAMPLE\n",
      "Number of instances in restricted sample: 957\n",
      "Instance distribution by class in restricted sample: \n",
      " Bacterial Pneumonia       422\n",
      "COVID-19                  313\n",
      "Viral Pneumonia           295\n",
      "No Pneumonia (healthy)    240\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if restriction == True:\n",
    "    \n",
    "    ############################\n",
    "    # > RESTRICTION  SUMMARY < #\n",
    "    ############################\n",
    "    print(\"SAMPLE RESTRICTION SUMMARY\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"ORIGINAL SAMPLE\")\n",
    "    print(F\"Original number of instances: {len(X_train)}\")\n",
    "    print(f\"Original instance distribution by class: \\n {pd.Series(y_train).value_counts()}\")\n",
    "    print(\"\")\n",
    "    print(\"RESTRICTED SAMPLE\")\n",
    "    print(f\"Number of instances in restricted sample: {len(restricted_sample)}\")\n",
    "    print(f\"Instance distribution by class in restricted sample: \\n {pd.Series(restricted_sample_labels).value_counts()}\")\n",
    "    \n",
    "    X_train = np.array(restricted_sample)\n",
    "    y_train = np.array(restricted_sample_labels)\n",
    "    \n",
    "else:\n",
    "    print(\"SAMPLE RESTRICTION WAS NOT CONDUCTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if augmentation_flip == True:\n",
    "\n",
    "    augmentedfeatures, augmentedlabels_ = augment_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE AUGMENTATION WAS NOT CONDUCTED\n"
     ]
    }
   ],
   "source": [
    "if augmentation_flip == True:\n",
    "    \n",
    "    ############################\n",
    "    # > AUGMENTATION SUMMARY < #\n",
    "    ############################\n",
    "    \n",
    "    print(\"SAMPLE AUGMENTATION SUMMARY\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"ORIGINAL SAMPLE\")\n",
    "    print(F\"Original number of instances: {len(labels)}\")\n",
    "    print(f\"Original instance distribution by class: \\n {pd.Series(labels).value_counts()}\")\n",
    "    print(\"\")\n",
    "    print(\"AUGMENTED SAMPLE\")\n",
    "    print(f\"Number of instances in augmented sample: {len(augmentedlabels)}\")\n",
    "    print(f\"Instance distribution by class in augmented sample: \\n {pd.Series(augmentedlabels).value_counts()}\")\n",
    "    \n",
    "    X_train = np.array(augmentedfeatures)\n",
    "    y_train = np.array(augmentedlabels)\n",
    "\n",
    "else:\n",
    "    print(\"SAMPLE AUGMENTATION WAS NOT CONDUCTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Categorical encoding of new labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a dictionary that maps each category to a numerical value\n",
    "#label_map = {\"Bacterial Pneumonia\": 0, \"Viral Pneumonia\": 1, \"No Pneumonia (healthy)\": 2, \"COVID-19\": 3}\n",
    "#\n",
    "## Encode the categorical labels as numerical values using the label map\n",
    "y_train_encoded = np.vectorize(label_map.get)(y_train)\n",
    "#y_val_encoded = np.vectorize(label_map.get)(y_val)\n",
    "#y_test_encoded = np.vectorize(label_map.get)(y_test)\n",
    "#\n",
    "## Convert the numerical labels to one-hot encoded format\n",
    "#num_classes = 4\n",
    "y_train_onehot = keras.utils.to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "#y_val_onehot = keras.utils.to_categorical(y_val_encoded, num_classes=num_classes)\n",
    "#y_test_onehot = keras.utils.to_categorical(y_test_encoded, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Network fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiDja0Mub4YW"
   },
   "outputs": [],
   "source": [
    "# First, let's find an optimum learning rate for the baseline model:\n",
    "# We will set less epochs to make it faster (epochs = 6)\n",
    "\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def lr_schedule(epoch, initial_lr, final_lr, total_epochs):\n",
    "    \"\"\"\n",
    "    calculates the learning rate for each epoch based on the initial learning rate, final learning rate, and total number of epochs\n",
    "    \"\"\"\n",
    "    lr = initial_lr + (final_lr - initial_lr) * (epoch / float(total_epochs))\n",
    "    return lr\n",
    "\n",
    "def plot_lr_schedule(initial_lr, final_lr, total_epochs):\n",
    "    lr = [lr_schedule(epoch, initial_lr, final_lr, total_epochs) for epoch in range(total_epochs)]\n",
    "    plt.plot(lr, history.history['val_loss'])\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.show()\n",
    "\n",
    "# Adding a Lr Scheduler to check the learning rate evolution during training and to avoid overfitting\n",
    "initial_lr = 0.002\n",
    "final_lr = 0.01\n",
    "baseline_epochs = 6\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(lambda epoch: lr_schedule(epoch, initial_lr, final_lr, baseline_epochs))\n",
    "\n",
    "# Train the model for 4 epochs with a batch size of 32\n",
    "history = baseline_model.fit(\n",
    "    X_train_norm,\n",
    "    y_train_onehot,\n",
    "    batch_size=32,\n",
    "    epochs=6,\n",
    "    validation_data=(X_val_norm, y_val_onehot),\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule of thumb: optimal will be a bit lower than when lr starts climbing, usually 10 times lower the climb up point (around 0.005)\n",
    "plot_lr_schedule(initial_lr, final_lr, baseline_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from keras import backend as K\n",
    "\n",
    "# Some memory clean-up\n",
    "K.clear_session()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNED MODEL 1: BASELINE + LR SCHEDULER + KFOLD VALIDATION + EARLY STOPPING\n",
    "\n",
    "def build_tuned_model_1():\n",
    "    model = keras.Sequential([\n",
    "    # Convolutional layers\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(156, 156, 3)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    # (CHANGE VS BASELINE) Adding another pack of Conv2D and MaxPooling2D layers\n",
    "    #keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    #keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    #keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  \n",
    "    # Dense layers\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    # (CHANGE VS BASELINE) Adding Dropout\n",
    "    #keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    # (CHANGE VS BASELINE) Adding Dropout\n",
    "    #keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(4, activation='softmax')])\n",
    "    # Compile the model with appropriate loss function, optimizer, and metrics\n",
    "    # (CHANGE VS BASELINE) Adding the optimal lr\n",
    "    optim = keras.optimizers.Adam(learning_rate=0.005) \n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K FOLD VALIDATION (5 max epochs for speeding purposes)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k = 4\n",
    "num_val_samples = len(X_train_norm) // k \n",
    "num_epochs = 5\n",
    "all_val_losses = [] # Should add the score of each run at the end of the loop\n",
    "all_val_accuracies = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = X_train_norm[i * num_val_samples: (i + 1) * num_val_samples] \n",
    "    val_targets = y_train_onehot[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [X_train_norm[:i * num_val_samples],\n",
    "         X_train_norm[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [y_train_onehot[:i * num_val_samples],\n",
    "         y_train_onehot[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    # (CHANGE VS BASELINE)Defining EarlyStopping callback\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "    tuned_model_1 = build_tuned_model_1()\n",
    "    # Train the model for 4 epochs with a batch size of 32\n",
    "    history = tuned_model_1.fit(\n",
    "        partial_train_data,\n",
    "        partial_train_targets,\n",
    "        batch_size=32,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(val_data, val_targets),\n",
    "        callbacks=[early_stopping]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the kfold results\n",
    "val_loss, val_accuracy = tuned_model_1.evaluate(val_data, val_targets, verbose=0)\n",
    "all_val_losses.append(val_loss)\n",
    "all_val_accuracies.append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_val_losses)\n",
    "print(all_val_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Analyze performance of fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBGr0x8ZcFn9"
   },
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fn_WnCfDcFyD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "160d3d50f83ed3d75d0672b6b4d1134bc09c3e08300a8645f571fa0ede095231"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
